<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NeON AI Chat Assistant</title>
    <!-- Load Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        /* Custom styles for the NeON Black & Green Dark Mode Theme */
        :root {
            --neon-green: #10b981; /* Emerald 500 */
            --neon-glow: 0 0 5px var(--neon-green), 0 0 10px var(--neon-green);
        }

        body {
            /* 1. Set the uploaded image (Neon Skull) as the fixed background */
            background-color: #0a0a0a;
            /* Using the content fetch ID for reliable loading of the uploaded image */
            background-image: url('uploaded:download (3).jfif-bcc2e9d4-75ea-4ec0-a59d-841fe766affe'); 
            background-size: cover;
            background-position: center;
            background-attachment: fixed; /* Keeps background stationary */
            font-family: 'Inter', sans-serif;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            padding: 1rem;
            color: #e5e7eb;
            position: relative; /* Needed for the overlay */
            z-index: 1; 
        }

        /* 2. Add a dark overlay to ensure text contrast over the bright skull */
        body::before {
            content: '';
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0, 0, 0, 0.7); /* 70% opacity black overlay */
            backdrop-filter: blur(2px); /* Subtle blur to the image */
            z-index: -1; /* Place behind chat content */
        }

        .chat-container {
            max-width: 768px;
            width: 100%;
            height: 90vh; /* Increased height */
            display: flex;
            flex-direction: column;
            box-shadow: 0 0 20px rgba(16, 185, 129, 0.2); /* Subtle glow effect */
            border-radius: 1.5rem;
            overflow: hidden;
            background-color: #111827; /* Very Dark container background */
            transition: all 0.3s ease;
        }
        .message-area {
            flex-grow: 1;
            padding: 1rem;
            overflow-y: auto;
            scroll-behavior: smooth;
            background-color: #0d121c; /* Even darker chat area */
        }

        /* Message Bubbles */
        .user-bubble {
            background-color: var(--neon-green);
            color: #000;
            font-weight: 600;
            align-self: flex-end;
            border-radius: 1.25rem 1.25rem 0.25rem 1.25rem;
            max-width: 80%;
            margin-bottom: 0.5rem;
            padding: 0.75rem 1rem;
            box-shadow: 0 2px 5px rgba(16, 185, 129, 0.5);
        }
        .ai-bubble {
            background-color: #2c3e50; /* Dark blue-gray for AI */
            color: #e5e7eb;
            align-self: flex-start;
            border-radius: 1.25rem 1.25rem 1.25rem 0.25rem;
            max-width: 80%;
            margin-bottom: 0.5rem;
            padding: 0.75rem 1rem;
        }
        .summary-bubble {
            background-color: #0f3d3d; /* Dark Teal for summary */
            color: #34d399; /* Light Teal text */
            border: 1px solid var(--neon-green);
            align-self: center;
            border-radius: 0.75rem;
            max-width: 90%;
            margin: 1rem 0;
            padding: 1rem;
            text-align: center;
            font-style: italic;
            box-shadow: var(--neon-glow);
        }

        /* Input and Button Styles */
        #user-input, .input-group button, #summarize-button, #image-input-label, #voice-input-button {
            transition: all 0.3s ease;
        }

        #user-input:focus, #voice-input-field:focus {
            box-shadow: var(--neon-glow);
            border-color: var(--neon-green) !important;
        }
        
        /* Neon Button Glow */
        .neon-button {
            box-shadow: 0 0 5px rgba(16, 185, 129, 0.5);
        }
        .neon-button:hover {
            box-shadow: 0 0 10px var(--neon-green), 0 0 20px rgba(16, 185, 129, 0.5);
            transform: translateY(-1px);
        }

        /* Scrollbar */
        .message-area::-webkit-scrollbar { width: 8px; }
        .message-area::-webkit-scrollbar-thumb { background-color: #4b5563; border-radius: 4px; }
        .message-area::-webkit-scrollbar-track { background: #1f2937; }

        /* Loading Dots */
        .loading-dot {
            width: 8px; height: 8px; background-color: #9ca3af; border-radius: 50%;
            display: inline-block; margin: 0 2px;
            animation: dot-loading 1.4s infinite ease-in-out both;
        }
        .loading-dot:nth-child(1) { animation-delay: -0.32s; }
        .loading-dot:nth-child(2) { animation-delay: -0.16s; }
        @keyframes dot-loading {
            0%, 80%, 100% { transform: scale(0); }
            40% { transform: scale(1.0); }
        }
        
        /* Link color */
        .ai-bubble a {
            color: #34d399; /* Light green link */
            text-decoration: underline;
        }

        /* Image Preview Styles */
        .image-preview-container {
            border: 1px solid #10b981;
            padding: 8px;
            border-radius: 0.75rem;
            background-color: #1e293b;
            position: relative;
            margin-bottom: 0.5rem;
            display: flex;
            align-items: center;
        }
        .image-preview-container img {
            max-width: 80px;
            max-height: 80px;
            border-radius: 0.5rem;
            object-fit: cover;
            margin-right: 12px;
        }
        .clear-image-button {
            position: absolute;
            top: -10px;
            right: -10px;
            background-color: #ef4444; /* Red */
            color: white;
            border-radius: 50%;
            width: 24px;
            height: 24px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 14px;
            font-weight: bold;
            line-height: 1;
            cursor: pointer;
            box-shadow: 0 0 5px #ef4444;
        }
        .clear-image-button:hover {
            background-color: #dc2626;
            box-shadow: 0 0 10px #dc2626;
        }

        /* Modal Styles */
        .modal {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0, 0, 0, 0.8);
            display: flex;
            justify-content: center;
            align-items: center;
            z-index: 100;
        }
        .modal-content {
            background-color: #1e293b;
            padding: 1.5rem;
            border-radius: 1rem;
            max-width: 400px;
            width: 90%;
            box-shadow: 0 0 15px var(--neon-green);
        }

        /* Microphone Animation */
        .listening-mic {
            animation: pulse-glow 1.5s infinite;
        }
        @keyframes pulse-glow {
            0% { transform: scale(1); opacity: 1; }
            50% { transform: scale(1.1); opacity: 0.7; }
            100% { transform: scale(1); opacity: 1; }
        }
    </style>
</head>
<body>

    <div class="chat-container">
        <!-- Header -->
        <div class="p-4 border-b border-gray-700 bg-gray-900">
            <div class="flex justify-between items-center mb-1">
                <h1 class="text-2xl font-bold text-gray-100 flex items-center">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6 mr-2 text-emerald-400" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                        <path d="M12 2a3 3 0 0 0-3 3v7a3 3 0 0 0 6 0V5a3 3 0 0 0-3-3z"></path>
                        <path d="M19 10v2a7 7 0 0 1-14 0v-2"></path>
                        <line x1="12" y1="19" x2="12" y2="22"></line>
                    </svg>
                    NeON AI (Urdu/English)
                </h1>
                <button
                    id="summarize-button"
                    onclick="summarizeChat()"
                    class="neon-button bg-emerald-700 hover:bg-emerald-600 text-white font-semibold py-2 px-3 rounded-xl transition duration-150 ease-in-out disabled:opacity-50 disabled:cursor-not-allowed text-sm flex items-center"
                >
                    ✨ Summarize Chat
                </button>
            </div>
            <p class="text-sm text-gray-400">Ask me anything in English or Urdu!</p>
        </div>

        <!-- Message Area -->
        <div id="message-area" class="message-area flex flex-col space-y-4">
            <!-- Initial Greeting -->
            <div class="ai-bubble">
                Hello! I am NeON AI, your advanced multimodal assistant. You can speak or type in **English** or **Urdu**!
            </div>
        </div>

        <!-- Input Area -->
        <div class="p-4 border-t border-gray-700 bg-gray-900">
            <div id="error-message" class="text-red-400 text-sm mb-2 hidden"></div>
            
            <!-- Image Preview -->
            <div id="image-preview-area" class="hidden">
                <!-- Preview elements will be injected here -->
            </div>

            <div class="flex space-x-3 input-group">
                <!-- Voice Input Button -->
                <button 
                    id="voice-input-button" 
                    onclick="tryNativeSpeechRecognition()"
                    class="neon-button bg-gray-700 hover:bg-gray-600 text-emerald-400 font-semibold py-3 px-3 rounded-xl transition duration-150 ease-in-out shadow-md cursor-pointer flex items-center justify-center"
                    title="Voice Input (Microphone)"
                >
                    <svg id="mic-icon" xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                        <path d="M12 1a3 3 0 0 0-3 3v8a3 3 0 0 0 6 0V4a3 3 0 0 0-3-3z"></path>
                        <path d="M19 10v2a7 7 0 0 1-14 0v-2"></path>
                        <line x1="12" y1="19" x2="12" y2="22"></line>
                    </svg>
                </button>

                <!-- Image Upload Button (Existing) -->
                <input type="file" id="image-input" accept="image/jpeg,image/png,image/webp" class="hidden" onchange="previewImage(event)">
                <label 
                    for="image-input" 
                    id="image-input-label"
                    class="neon-button bg-gray-700 hover:bg-gray-600 text-emerald-400 font-semibold py-3 px-3 rounded-xl transition duration-150 ease-in-out shadow-md cursor-pointer flex items-center justify-center"
                    title="Attach Image"
                >
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                        <rect x="3" y="3" width="18" height="18" rx="2" ry="2"></rect>
                        <circle cx="8.5" cy="8.5" r="1.5"></circle>
                        <polyline points="21 15 16 10 5 21"></polyline>
                    </svg>
                </label>

                <!-- Text Input -->
                <input
                    type="text"
                    id="user-input"
                    placeholder="Type your message or ask about the image (Urdu/English)..."
                    class="flex-1 p-3 border border-gray-600 bg-gray-800 text-gray-100 rounded-xl focus:ring-emerald-500 focus:border-emerald-500 transition duration-150"
                    onkeydown="if(event.key === 'Enter') sendMessage()"
                >
                <!-- Send Button -->
                <button
                    id="send-button"
                    onclick="sendMessage()"
                    class="neon-button bg-emerald-600 hover:bg-emerald-700 text-black font-semibold py-3 px-5 rounded-xl transition duration-150 ease-in-out shadow-md disabled:opacity-50 disabled:cursor-not-allowed flex items-center justify-center"
                >
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                        <line x1="22" y1="2" x2="11" y2="13"></line>
                        <polygon points="22 2 15 22 11 13 2 9 22 2"></polygon>
                    </svg>
                </button>
            </div>
        </div>
    </div>

    <!-- Voice Input Modal (Used for Native and Fallback) -->
    <div id="voice-modal" class="modal hidden">
        <div class="modal-content">
            <h2 id="modal-title" class="text-xl font-bold text-emerald-400 mb-3">🎙️ Voice Input</h2>
            <p id="modal-info" class="text-sm text-gray-300 mb-4">
                Click "Start Listening" and speak into your microphone.
            </p>
            <div id="native-voice-area" class="text-center">
                <p id="stt-status" class="text-lg font-semibold text-white mb-4">Ready.</p>
                <div class="flex justify-center items-center space-x-4">
                    <button 
                        id="stt-toggle"
                        onclick="toggleSpeechRecognition()"
                        class="neon-button bg-emerald-600 hover:bg-emerald-700 text-black font-semibold py-2 px-4 rounded-lg"
                    >
                        Start Listening
                    </button>
                    <button 
                        id="stt-send"
                        onclick="processVoiceInput(true)"
                        class="bg-gray-700 hover:bg-gray-600 text-white font-semibold py-2 px-4 rounded-lg disabled:opacity-50"
                        disabled
                    >
                        Send Command
                    </button>
                </div>
                <input 
                    type="text" 
                    id="voice-input-field" 
                    placeholder="Transcription appears here..."
                    class="mt-4 w-full p-3 border border-gray-600 bg-gray-800 text-gray-100 rounded-lg"
                    readonly
                >
            </div>
            <div id="fallback-voice-area" class="hidden">
                <input 
                    type="text" 
                    id="fallback-input-field" 
                    placeholder="Type your intended voice command here..."
                    class="w-full p-3 border border-gray-600 bg-gray-800 text-gray-100 rounded-lg focus:ring-emerald-500 focus:border-emerald-500 transition duration-150"
                    onkeydown="if(event.key === 'Enter') processVoiceInput(true, true)"
                >
                <div class="flex justify-end space-x-3 mt-4">
                    <button 
                        onclick="closeVoiceInputModal()"
                        class="bg-gray-700 hover:bg-gray-600 text-white font-semibold py-2 px-4 rounded-lg"
                    >
                        Cancel
                    </button>
                    <button 
                        onclick="processVoiceInput(true, true)"
                        class="neon-button bg-emerald-600 hover:bg-emerald-700 text-black font-semibold py-2 px-4 rounded-lg"
                    >
                        Use Command
                    </button>
                </div>
            </div>
        </div>
    </div>

    <!-- TTS Loading Modal -->
    <div id="tts-loading-modal" class="modal hidden">
        <div class="modal-content text-center">
            <h2 class="text-xl font-bold text-emerald-400 mb-4">🎧 Preparing Audio...</h2>
            <div class="flex justify-center">
                <div class="loading-dot" style="background-color: #34d399;"></div>
                <div class="loading-dot" style="background-color: #34d399;"></div>
                <div class="loading-dot" style="background-color: #34d399;"></div>
            </div>
            <p class="text-sm text-gray-400 mt-4">Generating speech using the AI voice model.</p>
        </div>
    </div>


    <script>
        const messageArea = document.getElementById('message-area');
        const userInput = document.getElementById('user-input');
        const sendButton = document.getElementById('send-button');
        const summarizeButton = document.getElementById('summarize-button');
        const imageInput = document.getElementById('image-input');
        const imagePreviewArea = document.getElementById('image-preview-area');
        const errorMessageDiv = document.getElementById('error-message');
        const voiceModal = document.getElementById('voice-modal');
        const modalTitle = document.getElementById('modal-title');
        const modalInfo = document.getElementById('modal-info');
        const sttStatus = document.getElementById('stt-status');
        const sttToggle = document.getElementById('stt-toggle');
        const sttSend = document.getElementById('stt-send');
        const voiceInputField = document.getElementById('voice-input-field');
        const micIcon = document.getElementById('mic-icon');
        const ttsLoadingModal = document.getElementById('tts-loading-modal');

        // Speech Recognition Setup
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        let recognition = null;
        let isListening = false;
        let finalTranscript = '';
        
        // --- Gemini API Configuration ---
        // API key is removed from the URL entirely. The platform handles authorization internally.
        const modelName = "gemini-2.5-flash-preview-09-2025";
        const ttsModelName = "gemini-2.5-flash-preview-tts";
        
        const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/${modelName}:generateContent`;
        const ttsApiUrl = `https://generativelanguage.googleapis.com/v1beta/models/${ttsModelName}:generateContent`;

        const systemPrompt = "You are 'NeON AI', an ultra-friendly, concise, and highly capable multimodal assistant. You can answer general questions using Google Search and also analyze and respond to images provided by the user. If the user speaks or types in Urdu, respond in clear, conversational Urdu. Otherwise, respond in English. Always keep your responses concise, conversational, and helpful.";
        
        let chatHistory = [];
        let isProcessing = false;

        /**
         * Generic fetch function with exponential backoff for API calls.
         * @param {string} url - The API endpoint URL (without key parameter).
         * @param {object} payload - The JSON payload for the POST request.
         * @param {number} maxRetries - Maximum number of retry attempts.
         * @returns {Promise<object>} The JSON response from the API.
         */
        async function apiFetch(url, payload, maxRetries = 4) { // Increased max retries to 4
            let result;
            let lastError = null;
            let delay = 1000;

            for (let i = 0; i < maxRetries; i++) {
                try {
                    const response = await fetch(url, {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify(payload)
                    });

                    if (response.ok) { return await response.json(); } 
                    else {
                        const errorBody = await response.json();
                        // Capture the specific error message for better debugging
                        lastError = errorBody.error ? errorBody.error.message : `API returned status ${response.status}`;
                        if (i < maxRetries - 1) { 
                            // Wait with exponential backoff before retrying
                            await new Promise(resolve => setTimeout(resolve, delay)); 
                            delay *= 2; 
                        }
                    }
                } catch (err) {
                    lastError = err.message || 'Network error during fetch';
                    if (i < maxRetries - 1) { 
                        await new Promise(resolve => setTimeout(resolve, delay)); 
                        delay *= 2; 
                    }
                }
            }
            throw new Error(lastError || "Failed to get a response from the API after multiple retries.");
        }


        // --- TTS Utility Functions ---

        /**
         * Converts a Base64 string to an ArrayBuffer.
         */
        function base64ToArrayBuffer(base64) {
            const binaryString = atob(base64);
            const len = binaryString.length;
            const bytes = new Uint8Array(len);
            for (let i = 0; i < len; i++) {
                bytes[i] = binaryString.charCodeAt(i);
            }
            return bytes.buffer;
        }

        /**
         * Converts signed 16-bit PCM audio data to a WAV Blob.
         * Assumes sample rate is 16000 Hz, 1 channel.
         */
        function pcmToWav(pcm16, sampleRate = 16000) {
            const numChannels = 1;
            const bytesPerSample = 2; // 16-bit PCM
            const blockAlign = numChannels * bytesPerSample;
            const byteRate = sampleRate * blockAlign;

            const buffer = new ArrayBuffer(44 + pcm16.length * bytesPerSample);
            const view = new DataView(buffer);

            // RIFF chunk descriptor
            writeString(view, 0, 'RIFF'); // Chunk ID
            view.setUint32(4, 36 + pcm16.length * bytesPerSample, true); // Chunk Size
            writeString(view, 8, 'WAVE'); // Format

            // FMT sub-chunk
            writeString(view, 12, 'fmt '); // Sub-chunk 1 ID
            view.setUint32(16, 16, true); // Sub-chunk 1 Size (16 for PCM)
            view.setUint16(20, 1, true); // Audio Format (1 = PCM)
            view.setUint16(22, numChannels, true); // Num Channels
            view.setUint32(24, sampleRate, true); // Sample Rate
            view.setUint32(28, byteRate, true); // Byte Rate
            view.setUint16(32, blockAlign, true); // Block Align
            view.setUint16(34, 16, true); // Bits Per Sample

            // DATA sub-chunk
            writeString(view, 36, 'data'); // Sub-chunk 2 ID
            view.setUint32(40, pcm16.length * bytesPerSample, true); // Sub-chunk 2 Size

            // Write PCM data
            let offset = 44;
            for (let i = 0; i < pcm16.length; i++) {
                view.setInt16(offset, pcm16[i], true);
                offset += bytesPerSample;
            }

            return new Blob([buffer], { type: 'audio/wav' });
        }

        function writeString(view, offset, string) {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        }

        /**
         * Converts text to speech using the Gemini TTS API and plays it.
         * NOTE: Using a single stable voice (Kore) to prevent auth/config errors.
         */
        async function readTextAloud(text, targetElement) {
            if (!text || isProcessing) return;

            // Simplified voice config for stability
            const voiceName = 'Kore'; // Stable English voice

            ttsLoadingModal.classList.remove('hidden');
            
            const speakButton = targetElement.querySelector('.speak-button');
            if(speakButton) {
                speakButton.disabled = true;
                speakButton.innerHTML = '🔊 Speaking...';
            }

            try {
                // Simple text cleaning for TTS (remove markdown bolding)
                const cleanText = text.replace(/(\*\*|__)/g, '');

                const payload = {
                    contents: [{
                        parts: [{ text: cleanText }]
                    }],
                    generationConfig: {
                        responseModalities: ["AUDIO"],
                        speechConfig: {
                            voiceConfig: {
                                prebuiltVoiceConfig: { voiceName: voiceName }
                            }
                        }
                    }
                };
                
                // Use the shared apiFetch function for resilience
                const result = await apiFetch(ttsApiUrl, payload);

                const part = result?.candidates?.[0]?.content?.parts?.[0];
                const audioData = part?.inlineData?.data;
                const mimeTypeHeader = part?.inlineData?.mimeType;

                if (audioData && mimeTypeHeader && mimeTypeHeader.startsWith("audio/L16")) {
                    const sampleRateMatch = mimeTypeHeader.match(/rate=(\d+)/);
                    const sampleRate = sampleRateMatch ? parseInt(sampleRateMatch[1], 10) : 16000;
                    
                    const pcmData = base64ToArrayBuffer(audioData);
                    const pcm16 = new Int16Array(pcmData);
                    const wavBlob = pcmToWav(pcm16, sampleRate);
                    const audioUrl = URL.createObjectURL(wavBlob);
                    
                    const audio = new Audio(audioUrl);
                    audio.play();

                    audio.onended = () => {
                        if(speakButton) {
                            speakButton.disabled = false;
                            speakButton.innerHTML = '📞 Speak/کال'; 
                        }
                        URL.revokeObjectURL(audioUrl); // Clean up the Blob URL
                    };
                    
                    audio.onerror = (e) => {
                        console.error("Audio playback failed:", e);
                        throw new Error("Audio playback failed.");
                    };
                } else {
                    throw new Error("Invalid audio data received from TTS API.");
                }

            } catch (error) {
                console.error("TTS Error:", error);
                displayError("TTS Failed: " + (error.message || "Could not generate speech."));
                if(speakButton) {
                    speakButton.disabled = false;
                    speakButton.innerHTML = '❌ Fail';
                    setTimeout(() => speakButton.innerHTML = '📞 Speak/کال', 3000);
                }
            } finally {
                ttsLoadingModal.classList.add('hidden');
            }
        }

        // --- Voice Input Implementation ---

        function initSpeechRecognition() {
            if (SpeechRecognition) {
                recognition = new SpeechRecognition();
                recognition.continuous = false; // Only listen for a single utterance
                recognition.interimResults = true; // Show results as they are transcribed
                recognition.lang = 'en-US'; // Default to US English (Users can try other dialects if supported by browser)

                recognition.onstart = () => {
                    isListening = true;
                    finalTranscript = '';
                    sttStatus.textContent = "Listening... Speak now!";
                    sttToggle.textContent = "Stop Listening";
                    micIcon.classList.add('listening-mic');
                    sttSend.disabled = true;
                    voiceInputField.value = '';
                };

                recognition.onresult = (event) => {
                    let interimTranscript = '';
                    for (let i = event.resultIndex; i < event.results.length; ++i) {
                        const transcript = event.results[i][0].transcript;
                        if (event.results[i].isFinal) {
                            finalTranscript += transcript + ' ';
                        } else {
                            interimTranscript += transcript;
                        }
                    }
                    voiceInputField.value = finalTranscript + interimTranscript;
                    if (finalTranscript.trim().length > 0) {
                        sttSend.disabled = false;
                    }
                };

                recognition.onend = () => {
                    isListening = false;
                    sttStatus.textContent = finalTranscript.trim() ? "Finished. Ready to Send." : "Ready.";
                    sttToggle.textContent = "Start Listening";
                    micIcon.classList.remove('listening-mic');
                    if (finalTranscript.trim().length > 0) {
                        sttSend.disabled = false;
                    }
                };

                recognition.onerror = (event) => {
                    console.error("Speech Recognition Error:", event.error);
                    closeVoiceInputModal(); // Close native modal on error
                    showFallbackVoiceInput("Error: Microphone access denied or failed. Please type your command.");
                };
            }
        }

        function toggleSpeechRecognition() {
            if (!recognition) return;

            if (isListening) {
                recognition.stop();
            } else {
                try {
                    recognition.start();
                } catch (e) {
                    console.error("Failed to start STT:", e);
                    closeVoiceInputModal();
                    showFallbackVoiceInput("Failed to start voice recognition. Please type your command.");
                }
            }
        }

        function tryNativeSpeechRecognition() {
            if (isProcessing) return;
            clearError();

            if (SpeechRecognition) {
                document.getElementById('native-voice-area').classList.remove('hidden');
                document.getElementById('fallback-voice-area').classList.add('hidden');
                modalTitle.textContent = "🎙️ Voice Input (Microphone)";
                modalInfo.textContent = "Click 'Start Listening' and speak into your microphone. (Supports English/Urdu)";
                voiceModal.classList.remove('hidden');
                voiceInputField.value = '';
                sttStatus.textContent = "Ready.";
                sttSend.disabled = true;
                initSpeechRecognition(); 
            } else {
                showFallbackVoiceInput("Your browser does not fully support direct voice input. Please type your command.");
            }
        }

        function showFallbackVoiceInput(message) {
            document.getElementById('native-voice-area').classList.add('hidden');
            document.getElementById('fallback-voice-area').classList.remove('hidden');
            modalTitle.textContent = "⌨️ Voice Input (Fallback)";
            modalInfo.textContent = message;
            voiceModal.classList.remove('hidden');
            document.getElementById('fallback-input-field').value = '';
            document.getElementById('fallback-input-field').focus();
        }

        function closeVoiceInputModal() {
            if (isListening && recognition) {
                recognition.stop();
            }
            voiceModal.classList.add('hidden');
        }

        function processVoiceInput(shouldSend, isFallback = false) {
            let commandText = '';

            if (isFallback) {
                commandText = document.getElementById('fallback-input-field').value.trim();
            } else {
                commandText = finalTranscript.trim();
            }
            
            if (shouldSend && commandText) {
                closeVoiceInputModal();
                userInput.value = commandText;
                sendMessage();
            } else if (shouldSend) {
                // Using an inline modal/message instead of alert()
                displayError("Please speak or type a command first."); 
                setTimeout(clearError, 3000); 
            }
        }

        // --- Image Handling Utilities ---

        function fileToBase64(file) {
            return new Promise((resolve, reject) => {
                const reader = new FileReader();
                reader.onload = () => {
                    const base64Data = reader.result.split(',')[1];
                    resolve(base64Data);
                };
                reader.onerror = error => reject(error);
                reader.readAsDataURL(file);
            });
        }
        
        function clearImage() {
            imageInput.value = '';
            imagePreviewArea.innerHTML = '';
            imagePreviewArea.classList.add('hidden');
        }

        function previewImage(event) {
            const file = event.target.files[0];
            if (!file) return;

            if (file.size > 4 * 1024 * 1024) { 
                displayError("Image is too large (max 4MB). Please select a smaller file.");
                clearImage();
                return;
            }
            clearError();

            const reader = new FileReader();
            reader.onload = (e) => {
                const previewHtml = `
                    <div class="image-preview-container">
                        <img src="${e.target.result}" alt="Image Preview">
                        <span class="text-sm text-gray-400 overflow-hidden whitespace-nowrap text-ellipsis mr-8">${file.name}</span>
                        <div class="clear-image-button" onclick="clearImage()">×</div>
                    </div>
                `;
                imagePreviewArea.innerHTML = previewHtml;
                imagePreviewArea.classList.remove('hidden');
            };
            reader.readAsDataURL(file);
        }

        // --- Chat Utilities ---

        function markdownToHtml(text) {
            let html = text;
            html = html.replace(/\*\*(.*?)\*\*/g, '<strong>$1</strong>');
            html = html.replace(/__(.*?)__/g, '<strong>$1</strong>');
            html = html.replace(/\[([^\]]+)\]\(([^)]+)\)/g, '<a href="$2" target="_blank" class="text-emerald-400 hover:underline">$1</a>');
            // Basic list support (handle * at start of line)
            html = html.replace(/^\*\s*(.*)/gm, '<li>$1</li>');
            if (html.includes('<li>')) {
                // Wrap list items in ul tags, but only if they are preceded/followed by non-list content or start/end of string
                html = html.replace(/(\n|^)(<li>.*?<\/li>)+(\n|$)/gs, (match, before, listItems, after) => {
                    // Remove internal <br> tags within a list block for cleaner formatting
                    listItems = listItems.replace(/<br>/g, '');
                    return `${before}<ul>${listItems}</ul>${after}`;
                });
            }
            html = html.replace(/\n/g, '<br>');
            return html;
        }

        function setProcessing(state) {
            isProcessing = state;
            sendButton.disabled = state;
            userInput.disabled = state;
            imageInput.disabled = state;
            document.getElementById('voice-input-button').disabled = state;
            summarizeButton.disabled = state || chatHistory.filter(p => p.role !== 'system').length < 2;
            const label = document.getElementById('image-input-label');
            if (label) { label.style.pointerEvents = state ? 'none' : 'auto'; }
            if (state) { showLoadingIndicator(); } else { hideLoadingIndicator(); userInput.focus(); }
        }

        function displayMessage(text, sender, sources = []) {
            const wrapper = document.createElement('div');
            const bubble = document.createElement('div');

            if (sender === 'summary') {
                wrapper.className = 'flex justify-center';
                bubble.className = 'summary-bubble text-sm whitespace-pre-wrap';
                bubble.innerHTML = `<strong>Conversation Summary:</strong><br>${markdownToHtml(text)}`;
            } else {
                wrapper.className = `flex ${sender === 'user' ? 'justify-end' : 'justify-start'}`;
                bubble.className = `${sender === 'user' ? 'user-bubble' : 'ai-bubble'} max-w-4/5 text-sm whitespace-pre-wrap`;
                
                if (sender === 'ai') {
                    // Inject TTS button for AI messages
                    
                    // Simple text cleaning for TTS (remove markdown bolding)
                    const cleanText = text.replace(/(\*\*|__)/g, '');
                    
                    const ttsButton = document.createElement('button');
                    ttsButton.className = 'speak-button neon-button bg-gray-600 hover:bg-gray-700 text-emerald-400 p-1.5 rounded-full ml-2 flex-shrink-0';
                    ttsButton.innerHTML = '📞 Speak/کال'; 
                    ttsButton.title = 'Speak Aloud';
                    ttsButton.onclick = (e) => {
                        e.stopPropagation();
                        readTextAloud(cleanText, wrapper);
                    };

                    const textContentDiv = document.createElement('div');
                    textContentDiv.innerHTML = markdownToHtml(text);
                    textContentDiv.style.flexGrow = '1';

                    bubble.innerHTML = ''; // Clear bubble content before appending
                    bubble.style.display = 'flex';
                    bubble.style.alignItems = 'center';
                    
                    bubble.appendChild(textContentDiv);
                    bubble.appendChild(ttsButton);

                } else {
                    bubble.innerHTML = text;
                }
            }

            wrapper.appendChild(bubble);
            messageArea.appendChild(wrapper);

            if (sender === 'ai' && sources.length > 0) {
                const citationDiv = document.createElement('div');
                citationDiv.className = 'text-xs text-gray-500 mt-1 mb-3 ml-4 max-w-4/5';
                
                let sourceHtml = '<strong>Sources:</strong> ';
                sources.forEach((source, index) => {
                    if (source.uri && source.title) {
                        const sourceLink = `<a href="${source.uri}" target="_blank" class="text-emerald-400 hover:text-emerald-300 hover:underline">${source.title}</a>`;
                        sourceHtml += `${sourceLink}${index < sources.length - 1 ? ', ' : ''}`;
                    }
                });

                citationDiv.innerHTML = sourceHtml;
                messageArea.appendChild(citationDiv);
            }
            messageArea.scrollTop = messageArea.scrollHeight;
        }

        function showLoadingIndicator() {
            const wrapper = document.createElement('div');
            wrapper.id = 'loading-indicator';
            wrapper.className = 'flex justify-start';
            const bubble = document.createElement('div');
            bubble.className = 'ai-bubble max-w-4/5 text-sm';
            bubble.innerHTML = '<span class="loading-dot"></span><span class="loading-dot"></span><span class="loading-dot"></span>';
            wrapper.appendChild(bubble);
            messageArea.appendChild(wrapper);
            messageArea.scrollTop = messageArea.scrollHeight;
        }

        function hideLoadingIndicator() {
            const indicator = document.getElementById('loading-indicator');
            if (indicator) { indicator.remove(); }
        }

        function clearError() {
            errorMessageDiv.textContent = '';
            errorMessageDiv.classList.add('hidden');
        }

        function displayError(message) {
            errorMessageDiv.textContent = 'Error: ' + message;
            errorMessageDiv.classList.remove('hidden');
        }


        // --- Core Chat Feature ---

        async function sendMessage() {
            if (isProcessing) return;
            clearError();

            const prompt = userInput.value.trim();
            const imageFile = imageInput.files[0];
            
            if (prompt === '' && !imageFile) {
                displayError("Please enter a message or attach an image.");
                return;
            }

            let userMessage = prompt;
            let base64Image = null;
            let mimeType = null;
            
            if (imageFile) {
                userMessage = `(Image attached: ${imageFile.name}) ${prompt}`;
                displayMessage(userMessage, 'user');
                setProcessing(true);
                
                try {
                    base64Image = await fileToBase64(imageFile);
                    mimeType = imageFile.type;
                } catch (e) {
                    displayError("Failed to process image file.");
                    setProcessing(false);
                    return;
                }
                
                clearImage();
            } else {
                displayMessage(prompt, 'user');
                setProcessing(true);
            }
            
            userInput.value = '';

            const userParts = [];
            if (base64Image && mimeType) {
                userParts.push({
                    inlineData: { mimeType: mimeType, data: base64Image }
                });
            }
            if (prompt) {
                userParts.push({ text: prompt });
            }
            
            const contents = [...chatHistory, { role: "user", parts: userParts }];

            try {
                const payload = {
                    contents: contents,
                    tools: [{ "google_search": {} }],
                    systemInstruction: {
                        parts: [{ text: systemPrompt }]
                    },
                };

                // Use the shared apiFetch function
                const result = await apiFetch(apiUrl, payload);
                const candidate = result.candidates?.[0];

                let aiText = "I apologize, I was unable to generate a response.";
                let sources = [];
                
                if (candidate && candidate.content?.parts?.[0]?.text) {
                    aiText = candidate.content.parts[0].text;

                    const groundingMetadata = candidate.groundingMetadata;
                    if (groundingMetadata && groundingAttributions) {
                        sources = groundingMetadata.groundingAttributions
                            .map(attribution => ({
                                uri: attribution.web?.uri,
                                title: attribution.web?.title,
                            }))
                            .filter(source => source.uri && source.title);
                    }
                } else if (result.error) {
                    aiText = `An API error occurred: ${result.error.message}`;
                }

                displayMessage(aiText, 'ai', sources);
                
                // Only store parts with text for history (ignore inline data)
                const historyUserParts = userParts.filter(p => p.text).map(p => ({ text: p.text }));
                const historyModelParts = [{ text: aiText }];

                if (historyUserParts.length > 0) {
                    chatHistory.push({ role: "user", parts: historyUserParts });
                }
                chatHistory.push({ role: "model", parts: historyModelParts });

            } catch (error) {
                console.error("Critical AI error:", error);
                displayError(error.message || "Something went wrong while communicating with the AI. Please try again.");
                
                // If the user's message was only an image or failed to process, pop the last history item
                if (chatHistory.length > 0 && chatHistory[chatHistory.length - 1].role === 'user' && chatHistory[chatHistory.length - 1].parts.length === 0) {
                    chatHistory.pop();
                }

            } finally {
                setProcessing(false);
            }
        }

        /**
         * LLM-powered feature: Summarizes the entire chat history.
         */
        async function summarizeChat() {
            if (isProcessing || chatHistory.filter(p => p.role !== 'system').length < 2) return;
            clearError();
            clearImage(); 
            setProcessing(true);

            // Filter history to only include text for summarization
            const textOnlyHistory = chatHistory.map(turn => {
                const textPart = turn.parts.find(p => p.text);
                return {
                    role: turn.role,
                    parts: textPart ? [{ text: textPart.text }] : []
                };
            }).filter(turn => turn.parts.length > 0);
            
            const summarizationPrompt = "Based on the conversation history above (ignore any image references), please provide a concise, readable, and bulleted summary of the main topics discussed and the key takeaways.";
            
            const summarizationContents = [
                ...textOnlyHistory,
                { role: "user", parts: [{ text: summarizationPrompt }] }
            ];

            try {
                const payload = { contents: summarizationContents };

                // Use the shared apiFetch function
                const result = await apiFetch(apiUrl, payload);
                const candidate = result.candidates?.[0];

                let summaryText = "I apologize, I was unable to generate a summary.";
                
                if (candidate && candidate.content?.parts?.[0]?.text) {
                    summaryText = candidate.content.parts[0].text;
                } else if (result.error) {
                    summaryText = `An API error occurred: ${result.error.message}`;
                }

                displayMessage(summaryText, 'summary');

            } catch (error) {
                console.error("Critical Summarization error:", error);
                displayError(error.message || "Something went wrong while generating the summary. Please try again.");
                
            } finally {
                setProcessing(false);
            }
        }

        // Initialize state
        window.onload = () => {
            setProcessing(false); // Initial call to set button states correctly
            userInput.focus();
        };
    </script>
</body>
</html>
